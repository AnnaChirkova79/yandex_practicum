**Описание проекта:**

Интернет-магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. Необходимо обучить модель классифицировать комментарии на позитивные и негативные.
В нашем распоряжении набор данных с разметкой о токсичности правок. Необходимо построить модель со значением метрики качества F1 не меньше 0.75.

**Выводы:**

1.	Загрузила данные и провела их предобработку - после ознакомления с данными и их предобработки объявлен корпус текстов. Затем тексты были очищены и лемматизированы. Проверка соотношения классов показала, что данные несбалансированы. Токсичных комментариев всего 10,2% от всего датасета. Этот аспект учли в дальнейшем при обучении моделей (использован class_weight). Для векторизации текстов был использован TfidfVectorizer().
2.	обучила 3 модели с разными гиперпараметрами (LogisticRegression, DecisionTreeClassifier, RandomForestClassifier) и проверила их на валидационной выборке.
3.	Лучший показатель F1 оказался у модели LogisticRegression - 0,75101 на валидационной выборке.
4.	На тестировании лучшая модель показала F1 = 0.75495. Поскольку требовалось найти модель классификации комментариев на позитивные и негативные со значением метрики качества F1 >= 0.75, рекомендовать могу LogisticRegression.


**Навыки и инструменты:**

Python, Pandas, Matplotlib, NumPy, SciPy, предобработка данных, визуализация данных, лемматизация, работа с моделями работа с моделями решающее дерево, случайный лес, логистическая регрессия.

**Статус проекта:** Завершен.
